{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4d749260540d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defs\n",
    "\n",
    "def tokenize(corpus : str) -> list:\n",
    "    tokens = []\n",
    "    for sentence in corpus:\n",
    "        tokens.append(sentence.split())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(tokens):\n",
    "    vocabulary = []\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}    \n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_center_context_pair(tokens, window: int) -> dict:\n",
    "    pairs = dict()\n",
    "    for row in tokens:\n",
    "        for idx, center_word in enumerate(row):\n",
    "            pairs.setdefault(center_word, [])\n",
    "            for i in range(idx - window, idx + window + 1):\n",
    "                if (i >= 0 and i != idx and i < len(row)):\n",
    "                    pairs[center_word].append(row[i])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idxpairs(cc_pair: dict, w2idx: list) -> list:\n",
    "    \"\"\"\n",
    "    The generate_center_context_pair gives a dictionary like:\n",
    "    {'center word 1': ['contextword1', 'contextword2', '...']\n",
    "     'centerword2': ['contextword1', 'contextword2', '...']}\n",
    "    But the code from the blog needs cc_pair like:\n",
    "    [['centerword1', 'contextword1'],\n",
    "     ['centerword1', 'contextword2'], ...]\n",
    "    So this part changes from the former format to the latter\n",
    "    \"\"\"\n",
    "    idx_pairs = []\n",
    "    for center in cc_pair.keys():\n",
    "        for context in cc_pair[center]:\n",
    "            idx_pairs.append([w2idx[center], w2idx[context]])\n",
    "    return idx_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jdt(cc_pair: dict) -> list:\n",
    "    jdt = []\n",
    "    for center in cc_pair.keys():\n",
    "        for context in cc_pair[center]:\n",
    "            jdt.append([center, context])\n",
    "    return jdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_p_of_context_given_center(joint_distrib_table: pd.DataFrame):\n",
    "    counts = joint_distrib_table.groupby(['center', 'context']).size()\n",
    "    counts = counts.to_dict()\n",
    "\n",
    "    # Denominator for the probability\n",
    "    total = joint_distrib_table.groupby('center').size()\n",
    "    total = total.to_dict()\n",
    "\n",
    "    for center in total.keys():\n",
    "        for k in list(counts.keys()):\n",
    "            if k[0] is center:\n",
    "                counts[k] = [counts[k]]\n",
    "                counts[k].append(total[center])\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        \"he is a king\",\n",
    "        \"she is a queen\",\n",
    "        \"he is a man\",\n",
    "        \"she is a woman\",\n",
    "        \"warsaw is poland capital\",\n",
    "        \"berlin is germany capital\",\n",
    "        \"paris is france capital\",\n",
    "        # \"Sxi este juna kaj bela\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 0, 'is': 1, 'a': 2, 'king': 3, 'she': 4, 'queen': 5, 'man': 6, 'woman': 7, 'warsaw': 8, 'poland': 9, 'capital': 10, 'berlin': 11, 'germany': 12, 'paris': 13, 'france': 14}\n",
      "{0: 'he', 1: 'is', 2: 'a', 3: 'king', 4: 'she', 5: 'queen', 6: 'man', 7: 'woman', 8: 'warsaw', 9: 'poland', 10: 'capital', 11: 'berlin', 12: 'germany', 13: 'paris', 14: 'france'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-75d5cd717150>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-75d5cd717150>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0membedding_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     W1 = Variable(torch.randn(embedding_dims, vocab_size).float(),\n\u001b[0m\u001b[0;32m     18\u001b[0m             requires_grad=True)\n\u001b[0;32m     19\u001b[0m     W2 = Variable(torch.randn(vocab_size, embedding_dims).float(),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    tokens = tokenize(corpus)\n",
    "    vocabulary = set(sum(tokens, [])) # sum() flattens the 2d list\n",
    "    vocab_size = len(vocabulary)\n",
    "    cc_pair = generate_center_context_pair(tokens, 2)\n",
    "    # pprint(cc_pair)\n",
    "\n",
    "    word2idx = word2index(tokens)\n",
    "    idx2word = {key: val for (val, key) in word2idx.items()}\n",
    "    print(word2idx)\n",
    "    print(idx2word)\n",
    "\n",
    "    idx_pairs = get_idxpairs(cc_pair, word2idx)\n",
    "    idx_pairs = np.array(idx_pairs)\n",
    "\n",
    "    embedding_dims = 5\n",
    "    W1 = Variable(torch.randn(embedding_dims, vocab_size).float(),\n",
    "            requires_grad=True)\n",
    "    W2 = Variable(torch.randn(vocab_size, embedding_dims).float(),\n",
    "            requires_grad=True)\n",
    "    max_iter = 101\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        loss_val = 0\n",
    "        for data, target in idx_pairs:\n",
    "            x = Variable(get_input_layer(data, vocab_size)).float()\n",
    "            y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "            z1 = torch.matmul(W1, x)\n",
    "            z2 = torch.matmul(W2, z1)\n",
    "\n",
    "            log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "            loss = F.nll_loss(log_softmax.view(1, -1), y_true)\n",
    "            loss_val += loss.item()\n",
    "            loss.backward()\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Loss at iter {i}: {loss_val/len(idx_pairs)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
