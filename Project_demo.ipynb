{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defs\n",
    "\n",
    "def tokenize(corpus : str) -> list:\n",
    "    tokens = []\n",
    "    for sentence in corpus:\n",
    "        tokens.append(sentence.split())\n",
    "    return tokens\n",
    "\n",
    "def remove_stops(corpus, stop_words=[\"is\", \"a\"]):\n",
    "    c = []\n",
    "    for line in corpus:\n",
    "        s = \"\"\n",
    "        for word in line.split():\n",
    "            if word not in stop_words:\n",
    "                s += word + \" \"\n",
    "        c.append(s.strip())\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(tokens):\n",
    "    vocabulary = []\n",
    "    for sentence in tokens:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.append(token)\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}    \n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_center_context_pair(tokens, window: int) -> dict:\n",
    "    pairs = dict()\n",
    "    for row in tokens:\n",
    "        for idx, center_word in enumerate(row):\n",
    "            pairs.setdefault(center_word, [])\n",
    "            for i in range(idx - window, idx + window + 1):\n",
    "                if (i >= 0 and i != idx and i < len(row)):\n",
    "                    pairs[center_word].append(row[i])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idxpairs(cc_pair: dict, w2idx: list) -> list:\n",
    "    \"\"\"\n",
    "    The generate_center_context_pair gives a dictionary like:\n",
    "    {'center word 1': ['contextword1', 'contextword2', '...']\n",
    "     'centerword2': ['contextword1', 'contextword2', '...']}\n",
    "    But the code from the blog needs cc_pair like:\n",
    "    [['centerword1', 'contextword1'],\n",
    "     ['centerword1', 'contextword2'], ...]\n",
    "    So this part changes from the former format to the latter\n",
    "    \"\"\"\n",
    "    idx_pairs = []\n",
    "    for center in cc_pair.keys():\n",
    "        for context in cc_pair[center]:\n",
    "            idx_pairs.append([w2idx[center], w2idx[context]])\n",
    "    return idx_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jdt(cc_pair: dict) -> list:\n",
    "    jdt = []\n",
    "    for center in cc_pair.keys():\n",
    "        for context in cc_pair[center]:\n",
    "            jdt.append([center, context])\n",
    "    return jdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_p_of_context_given_center(joint_distrib_table: pd.DataFrame):\n",
    "    counts = joint_distrib_table.groupby(['center', 'context']).size()\n",
    "    counts = counts.to_dict()\n",
    "\n",
    "    # Denominator for the probability\n",
    "    total = joint_distrib_table.groupby('center').size()\n",
    "    total = total.to_dict()\n",
    "\n",
    "    for center in total.keys():\n",
    "        for k in list(counts.keys()):\n",
    "            if k[0] is center:\n",
    "                counts[k] = [counts[k]]\n",
    "                counts[k].append(total[center])\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        \"he is a king\",\n",
    "        \"she is a queen\",\n",
    "        \"he is a man\",\n",
    "        \"she is a woman\",\n",
    "        \"warsaw is poland capital\",\n",
    "        \"berlin is germany capital\",\n",
    "        \"paris is france capital\",\n",
    "        # \"Sxi este juna kaj bela\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_input_layer(word_idx, vocab_size):\n",
    "    x = torch.zeros(vocab_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'he': 0, 'is': 1, 'a': 2, 'king': 3, 'she': 4, 'queen': 5, 'man': 6, 'woman': 7, 'warsaw': 8, 'poland': 9, 'capital': 10, 'berlin': 11, 'germany': 12, 'paris': 13, 'france': 14}\n",
      "{0: 'he', 1: 'is', 2: 'a', 3: 'king', 4: 'she', 5: 'queen', 6: 'man', 7: 'woman', 8: 'warsaw', 9: 'poland', 10: 'capital', 11: 'berlin', 12: 'germany', 13: 'paris', 14: 'france'}\n",
      "Loss at iter 0: 3.787002455620539\n",
      "Loss at iter 10: 3.561468155611129\n",
      "Loss at iter 20: 3.391857780161358\n",
      "Loss at iter 30: 3.2646664636475697\n",
      "Loss at iter 40: 3.164361834526062\n",
      "Loss at iter 50: 3.0797781149546304\n",
      "Loss at iter 60: 3.0049213483220054\n",
      "Loss at iter 70: 2.9368412040528797\n",
      "Loss at iter 80: 2.874013046423594\n",
      "Loss at iter 90: 2.8155458030246554\n",
      "Loss at iter 100: 2.760840688432966\n",
      "Loss at iter 110: 2.7094494161151705\n",
      "Loss at iter 120: 2.6610090136528015\n",
      "Loss at iter 130: 2.6152140469778153\n",
      "Loss at iter 140: 2.5718014126732234\n",
      "Loss at iter 150: 2.530541317803519\n",
      "Loss at iter 160: 2.4912318757602145\n",
      "Loss at iter 170: 2.453694993541354\n",
      "Loss at iter 180: 2.4177740329787847\n",
      "Loss at iter 190: 2.383330770901271\n",
      "Center: woman ; Context: queen\n",
      "Center: poland ; Context: is\n",
      "Center: a ; Context: is\n",
      "Center: he ; Context: is\n",
      "Center: warsaw ; Context: is\n",
      "Center: germany ; Context: king\n",
      "Center: queen ; Context: germany\n",
      "Center: berlin ; Context: germany\n",
      "Center: france ; Context: germany\n",
      "Center: paris ; Context: berlin\n",
      "Center: capital ; Context: is\n",
      "Center: she ; Context: is\n",
      "Center: man ; Context: is\n",
      "Center: is ; Context: a\n",
      "Center: king ; Context: germany\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    tokens = tokenize(corpus)\n",
    "    vocabulary = set(sum(tokens, [])) # sum() flattens the 2d list\n",
    "    vocab_size = len(vocabulary)\n",
    "    cc_pair = generate_center_context_pair(tokens, 1)\n",
    "    # pprint(cc_pair)\n",
    "\n",
    "    word2idx = word2index(tokens)\n",
    "    idx2word = {key: val for (val, key) in word2idx.items()}\n",
    "    print(word2idx)\n",
    "    print(idx2word)\n",
    "\n",
    "    idx_pairs = get_idxpairs(cc_pair, word2idx)\n",
    "    idx_pairs = np.array(idx_pairs)\n",
    "\n",
    "    embedding_dims = 5\n",
    "    W1 = Variable(torch.randn(embedding_dims, vocab_size).float(),\n",
    "            requires_grad=True)\n",
    "    W2 = Variable(torch.randn(vocab_size, embedding_dims).float(),\n",
    "            requires_grad=True)\n",
    "    max_iter = 200\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        loss_val = 0\n",
    "        for data, target in idx_pairs:\n",
    "            x = Variable(get_input_layer(data, vocab_size)).float()\n",
    "            y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "            z1 = torch.matmul(W1, x)\n",
    "            z2 = torch.matmul(W2, z1)\n",
    "\n",
    "            log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "            loss = F.nll_loss(log_softmax.view(1, -1), y_true)\n",
    "            loss_val += loss.item()\n",
    "            loss.backward()\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            W2.grad.data.zero_()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Loss at iter {i}: {loss_val/len(idx_pairs)}\")\n",
    "\n",
    "    # Lets see the word predictions for each word in our vocabulary\n",
    "    for word in vocabulary:\n",
    "        widx = word2idx[word]\n",
    "        x = Variable(get_input_layer(widx, vocab_size)).float()\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "\n",
    "        softmax = F.softmax(z2, dim=0)\n",
    "        max_arg = torch.argmax(softmax).item()\n",
    "        pred_word = idx2word[max_arg]\n",
    "        print(f\"Center: {word} ; Context: {pred_word}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
